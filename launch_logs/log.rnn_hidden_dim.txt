*****
Namespace(batch_size=256, dropout_prob=0.0, emb_dim=300, epochs=5, hidden_dim=25, id='rnn_hidden_dim_25', kernel_size=3, log_interval=100, lr=0.0005, model='rnn', num_workers=8, seed=42, shuffle=1, train='/scratch/mt3685/nl_data/snli_train.tsv', use_cuda=1, val='/scratch/mt3685/nl_data/snli_val.tsv')

==================== epoch: 1 ====================
epoch: [ 1/ 5]; step: [101/391]; loss: 1.0790
epoch: [ 1/ 5]; step: [201/391]; loss: 1.0220
epoch: [ 1/ 5]; step: [301/391]; loss: 0.9951

 epoch: [ 1/ 5]; val acc: 52.7; val loss: 0.9809

==================== epoch: 2 ====================
epoch: [ 2/ 5]; step: [101/391]; loss: 0.8876
epoch: [ 2/ 5]; step: [201/391]; loss: 0.8823
epoch: [ 2/ 5]; step: [301/391]; loss: 0.8467

 epoch: [ 2/ 5]; val acc: 57.5; val loss: 0.9000

==================== epoch: 3 ====================
epoch: [ 3/ 5]; step: [101/391]; loss: 0.9085
epoch: [ 3/ 5]; step: [201/391]; loss: 0.9018
epoch: [ 3/ 5]; step: [301/391]; loss: 0.8573

 epoch: [ 3/ 5]; val acc: 59.2; val loss: 0.8749

==================== epoch: 4 ====================
epoch: [ 4/ 5]; step: [101/391]; loss: 0.8868
epoch: [ 4/ 5]; step: [201/391]; loss: 0.7824
epoch: [ 4/ 5]; step: [301/391]; loss: 0.8384

 epoch: [ 4/ 5]; val acc: 60.6; val loss: 0.8613

==================== epoch: 5 ====================
epoch: [ 5/ 5]; step: [101/391]; loss: 0.9159
epoch: [ 5/ 5]; step: [201/391]; loss: 0.8586
epoch: [ 5/ 5]; step: [301/391]; loss: 0.8606

 epoch: [ 5/ 5]; val acc: 61.2; val loss: 0.8497
*****

*****
Namespace(batch_size=256, dropout_prob=0.0, emb_dim=300, epochs=5, hidden_dim=50, id='rnn_hidden_dim_50', kernel_size=3, log_interval=100, lr=0.0005, model='rnn', num_workers=8, seed=42, shuffle=1, train='/scratch/mt3685/nl_data/snli_train.tsv', use_cuda=1, val='/scratch/mt3685/nl_data/snli_val.tsv')

==================== epoch: 1 ====================
epoch: [ 1/ 5]; step: [101/391]; loss: 1.0599
epoch: [ 1/ 5]; step: [201/391]; loss: 0.9972
epoch: [ 1/ 5]; step: [301/391]; loss: 0.9601

 epoch: [ 1/ 5]; val acc: 57.0; val loss: 0.9250

==================== epoch: 2 ====================
epoch: [ 2/ 5]; step: [101/391]; loss: 0.9170
epoch: [ 2/ 5]; step: [201/391]; loss: 0.8713
epoch: [ 2/ 5]; step: [301/391]; loss: 0.8067

 epoch: [ 2/ 5]; val acc: 59.0; val loss: 0.8741

==================== epoch: 3 ====================
epoch: [ 3/ 5]; step: [101/391]; loss: 0.8515
epoch: [ 3/ 5]; step: [201/391]; loss: 0.8481
epoch: [ 3/ 5]; step: [301/391]; loss: 0.8760

 epoch: [ 3/ 5]; val acc: 61.1; val loss: 0.8462

==================== epoch: 4 ====================
epoch: [ 4/ 5]; step: [101/391]; loss: 0.7584
epoch: [ 4/ 5]; step: [201/391]; loss: 0.8465
epoch: [ 4/ 5]; step: [301/391]; loss: 0.7914

 epoch: [ 4/ 5]; val acc: 61.8; val loss: 0.8348

==================== epoch: 5 ====================
epoch: [ 5/ 5]; step: [101/391]; loss: 0.7897
epoch: [ 5/ 5]; step: [201/391]; loss: 0.7640
epoch: [ 5/ 5]; step: [301/391]; loss: 0.7151

 epoch: [ 5/ 5]; val acc: 63.4; val loss: 0.8195
*****

*****
Namespace(batch_size=256, dropout_prob=0.0, emb_dim=300, epochs=5, hidden_dim=100, id='rnn_hidden_dim_100', kernel_size=3, log_interval=100, lr=0.0005, model='rnn', num_workers=8, seed=42, shuffle=1, train='/scratch/mt3685/nl_data/snli_train.tsv', use_cuda=1, val='/scratch/mt3685/nl_data/snli_val.tsv')

==================== epoch: 1 ====================
epoch: [ 1/ 5]; step: [101/391]; loss: 1.0572
epoch: [ 1/ 5]; step: [201/391]; loss: 0.9776
epoch: [ 1/ 5]; step: [301/391]; loss: 0.8613

 epoch: [ 1/ 5]; val acc: 56.0; val loss: 0.9081

==================== epoch: 2 ====================
epoch: [ 2/ 5]; step: [101/391]; loss: 0.8839
epoch: [ 2/ 5]; step: [201/391]; loss: 0.8474
epoch: [ 2/ 5]; step: [301/391]; loss: 0.8326

 epoch: [ 2/ 5]; val acc: 59.5; val loss: 0.8706

==================== epoch: 3 ====================
epoch: [ 3/ 5]; step: [101/391]; loss: 0.8021
epoch: [ 3/ 5]; step: [201/391]; loss: 0.8318
epoch: [ 3/ 5]; step: [301/391]; loss: 0.8474

 epoch: [ 3/ 5]; val acc: 61.2; val loss: 0.8368

==================== epoch: 4 ====================
epoch: [ 4/ 5]; step: [101/391]; loss: 0.7071
epoch: [ 4/ 5]; step: [201/391]; loss: 0.7922
epoch: [ 4/ 5]; step: [301/391]; loss: 0.7558

 epoch: [ 4/ 5]; val acc: 64.5; val loss: 0.8023

==================== epoch: 5 ====================
epoch: [ 5/ 5]; step: [101/391]; loss: 0.6861
epoch: [ 5/ 5]; step: [201/391]; loss: 0.7346
epoch: [ 5/ 5]; step: [301/391]; loss: 0.7445

 epoch: [ 5/ 5]; val acc: 65.4; val loss: 0.7799
*****

*****
Namespace(batch_size=256, dropout_prob=0.0, emb_dim=300, epochs=5, hidden_dim=250, id='rnn_hidden_dim_250', kernel_size=3, log_interval=100, lr=0.0005, model='rnn', num_workers=8, seed=42, shuffle=1, train='/scratch/mt3685/nl_data/snli_train.tsv', use_cuda=1, val='/scratch/mt3685/nl_data/snli_val.tsv')

==================== epoch: 1 ====================
epoch: [ 1/ 5]; step: [101/391]; loss: 0.9897
epoch: [ 1/ 5]; step: [201/391]; loss: 0.9331
epoch: [ 1/ 5]; step: [301/391]; loss: 0.9080

 epoch: [ 1/ 5]; val acc: 59.5; val loss: 0.8691

==================== epoch: 2 ====================
epoch: [ 2/ 5]; step: [101/391]; loss: 0.7831
epoch: [ 2/ 5]; step: [201/391]; loss: 0.8641
epoch: [ 2/ 5]; step: [301/391]; loss: 0.8742

 epoch: [ 2/ 5]; val acc: 64.2; val loss: 0.8189

==================== epoch: 3 ====================
epoch: [ 3/ 5]; step: [101/391]; loss: 0.7934
epoch: [ 3/ 5]; step: [201/391]; loss: 0.8283
epoch: [ 3/ 5]; step: [301/391]; loss: 0.7248

 epoch: [ 3/ 5]; val acc: 66.9; val loss: 0.7705

==================== epoch: 4 ====================
epoch: [ 4/ 5]; step: [101/391]; loss: 0.6992
epoch: [ 4/ 5]; step: [201/391]; loss: 0.7176
epoch: [ 4/ 5]; step: [301/391]; loss: 0.6913

 epoch: [ 4/ 5]; val acc: 66.2; val loss: 0.7563

==================== epoch: 5 ====================
epoch: [ 5/ 5]; step: [101/391]; loss: 0.8048
epoch: [ 5/ 5]; step: [201/391]; loss: 0.6279
epoch: [ 5/ 5]; step: [301/391]; loss: 0.6477

 epoch: [ 5/ 5]; val acc: 67.8; val loss: 0.7262
*****

